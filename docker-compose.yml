services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: backend
      cache_from:
        - german-ai-backend:latest
    image: german-ai-backend:latest
    container_name: german_backend
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11435
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - whisper
      - piper
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
  
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: frontend
      cache_from:
        - german-ai-frontend:latest
      args:
        - NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api/v1
    image: german-ai-frontend:latest
    container_name: german_frontend
    env_file:
      - .env
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped
  
  redis:
    image: redis:7-alpine
    container_name: german_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --tcp-backlog 511
      --timeout 0
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped
  
  ollama:
    image: ollama/ollama:latest
    container_name: german_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=1  # Reduce parallel requests to lower CPU
      - OLLAMA_MAX_LOADED_MODELS=1  # Keep only 1 model loaded
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Limit Ollama to 2 CPU cores to match local Docker CPU limit
        # NVIDIA GPU support for deployment (uncomment when deploying to GPU server)
        # reservations:
        #   devices:
        #     - driver: nvidia
        #       count: 1
        #       capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: german_whisper
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=tiny  # Use tiny model - fastest, good enough for German
      - ASR_ENGINE=faster_whisper
      - ASR_LANGUAGE=de
    volumes:
      - whisper_data:/root/.cache
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
  
  piper:
    image: rhasspy/wyoming-piper:latest
    container_name: german_piper
    ports:
      - "10200:10200"
    volumes:
      - piper_data:/data
    command: --voice de_DE-thorsten-high
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Limit Piper to 2 CPU cores to prevent spikes
    healthcheck:
      test: ["CMD-SHELL", "echo 'test' | nc -z localhost 10200 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

volumes:
  redis_data:
  ollama_data:
  whisper_data:
  piper_data:
