services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: backend
      cache_from:
        - german-ai-backend:latest
    image: german-ai-backend:latest
    container_name: german_backend
    env_file:
      - .env
    environment:
      # Native Ollama configuration (runs on host machine with GPU)
      - OLLAMA_HOST=http://host.docker.internal:11435
      - OLLAMA_MODEL=mistral:7b
      - OLLAMA_MODEL_FAST=llama3.2:3b
      - OLLAMA_MODEL_GRAMMAR=gemma2:9b
      # MongoDB Atlas connection (from .env)
      - MONGODB_URL=${MONGODB_URI}
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - whisper
      - piper
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    networks:
      - german-ai-network
  
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: frontend
      cache_from:
        - german-ai-frontend:latest
      args:
        - NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api/v1
    image: german-ai-frontend:latest
    container_name: german_frontend
    env_file:
      - .env
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - german-ai-network
  
  redis:
    image: redis:7-alpine
    container_name: german_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --tcp-backlog 511
      --timeout 0
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped
    networks:
      - german-ai-network
  
  # Ollama service is DISABLED by default
  # Ollama should run NATIVELY on your machine for GPU access
  # The setup.sh script will install and configure native Ollama automatically
  # 
  # If you really need to run Ollama in Docker (not recommended):
  # 1. Uncomment the service below
  # 2. Update OLLAMA_HOST in .env to http://ollama:11434
  # 3. Note: Docker Ollama won't have GPU access unless you configure it
  
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: german_ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #     - OLLAMA_KEEP_ALIVE=24h
  #     - OLLAMA_NUM_PARALLEL=1
  #     - OLLAMA_MAX_LOADED_MODELS=1
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '2.0'
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/"]
  #     interval: 30s
  #     timeout: 30s
  #     retries: 5
  #     start_period: 120s

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: german_whisper
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=tiny  # Use tiny model - fastest, good enough for German
      - ASR_ENGINE=faster_whisper
      - ASR_LANGUAGE=de
    volumes:
      - whisper_data:/root/.cache
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - german-ai-network
  
  piper:
    image: rhasspy/wyoming-piper:latest
    container_name: german_piper
    ports:
      - "10200:10200"
    volumes:
      - piper_data:/data
    command: --voice de_DE-thorsten-high
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Limit Piper to 2 CPU cores to prevent spikes
    healthcheck:
      test: ["CMD-SHELL", "echo 'test' | nc -z localhost 10200 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - german-ai-network

networks:
  german-ai-network:
    driver: bridge

volumes:
  redis_data:
  ollama_data:
  whisper_data:
  piper_data:
